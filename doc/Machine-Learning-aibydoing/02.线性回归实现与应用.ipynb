{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 线性回归实现与应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 一元线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([56, 72, 69, 88, 102, 86, 76, 79, 94, 74])\n",
    "y = np.array([92, 102, 86, 110, 130, 99, 96, 102, 105, 92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "custom_params = {\"figure.figsize\": (6, 4),\n",
    "                 \"font.sans-serif\":\"Arial Unicode MS\",\n",
    "                 'axes.unicode_minus': False}\n",
    "sns.set_theme(style=\"ticks\", font_scale=0.7, rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Price')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFuCAYAAAA795qmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdy0lEQVR4nO3df0yd9f338RccbEEpRQ4cBacxpHMbFpx1p7UM+kNalVqhODPdWMTo2uAftlPjRkcbnd4LLlm+kWmcqSYTN4eabKUjqclmtQNHqcd9VWBMrcU2cdByhBYEivScc91/7PbsbitH2IdzXedwno9kyc7FR3ifKyfw7HVd5zpJlmVZAgAA+C8lOz0AAACIb8QEAAAwQkwAAAAjxAQAADBCTAAAACPEBAAAMEJMAAAAIylODxBt3/rWtzQ1NaWcnBynRwEAIK74/X4tWLBAb731VsR18z4mPvvsMwWDQafHAAAg7gQCAc3k3pbzPiY8Ho8kad++fQ5PAgBAfCkrK5vROq6ZAAAARogJAABghJgAAABGiAkAAGCEmAAAAEaICQAAYISYAAAARub9fSYAAJjvgiFLvX1DGh6dVFZGqgry3XIlJ9n284kJAADiWEdXv3a1dGtoZDK8zb04VVs2Faq4KM+WGWw/zTExMaGNGzdqeHhYkvSLX/xCGzZsUGVlpV5//fXwutbWVlVUVKiqqkp79+61e0wAAGJeR1e/Gpp8Z4SEJA2NTKqhyaeOrn5b5rD1yMThw4dVX1+vI0eOSJLeeOMN9fb2qrW1VaOjo6qoqND+/fvl9/vV2NiolpYWWZalqqoqrVq1Sunp6XaOCwBAzAqGLO1q6Y645pk9PVqxNDfqpzxsjYnm5mZt27ZN9fX1kqSSkhItX75cLpdLfr9fExMTCgaDOnDggEpLS8Px4PV61dnZqXXr1n3h94107/CBgQHl5ubO/ZMBAMBBvX1D5xyRONsnJ0+pt29IhUuyozqLrTGxY8eOc7YtWLBAjz32mF544QX96Ec/0oIFC3T8+PEzPjLc4/HI7/fbOSoAADFteDRySMx2nYmYuACzrq5O99xzj+666y6tXr1aKSkpCoVC4a8HAgEFAoFp//tInwg60088AwAgnmRlpM7pOhOO3mfiww8/1HvvvSdJWrx4sZYvX64333xTOTk5GhwcDK/z+/3hjxIHAABSQb5b7sWRQyE7M00F+e6oz+JoTBw5ckQ///nPFQgENDY2pjfeeENXXnmlSkpK1N7ervHxcZ04cUI+n09er9fJUQEAiCmu5CRt2VQYcc3myqW23G/C0dMc69at0zvvvKPKykolJyfr+9//vq666ipJUm1traqrqxUMBlVXV6esrCwnRwUAIOYUF+Vpe433nPtMZGemaXPlUtvuM5FkWZZly09yyOfXTES6rgIAgHgWrTtgzvRvaExcgAkAAP57ruSkqL/9MxI+6AsAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIAR22NiYmJCGzdu1PDwsCTp8ccf18aNG7VhwwY999xz4XWtra2qqKhQVVWV9u7da/eYAABghlLs/GGHDx9WfX29jhw5Iklqa2vTu+++q5aWFk1NTenWW29VaWmpLrjgAjU2NqqlpUWWZamqqkqrVq1Senq6neMCAIAZsDUmmpubtW3bNtXX10uSLr74Yt13331KSUlRSkqKLrvsMg0MDMjv96u0tDQcD16vV52dnVq3bt0Xft+ysrJpf+bAwIByc3Pn/skAAABJNsfEjh07znh8xRVXhP9/V1eX/vGPf2jZsmV6/vnnlZOTE/6ax+OR3++3bU4AADBztsbEdLq7u1VbW6vHHntM559/vlJSUhQKhcJfDwQCCgQC0/73+/btm/ZrkY5aAAAAc46/m8Pn8+mee+5RQ0ODVq1aJUnKycnR4OBgeI3f75fH43FqRAAAEIGjMfHRRx/pvvvu0xNPPKHVq1eHt5eUlKi9vV3j4+M6ceKEfD6fvF6vg5MCAIDpOHqao6mpSZOTk3r44YfD2+6//36tXr1atbW1qq6uVjAYVF1dnbKyspwbFAAATCvJsizL6SGi6fNrJiJdVwEAAM4107+hjl8zAQAA4hsxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMBIitMDAADMBEOWevuGNDw6qayMVBXku+VKTnJ6LCQQ22NiYmJC3/3ud/X8888rKytr2m2tra165pln5HK5tHnzZm3YsMHuUQEg5nV09WtXS7eGRibD29yLU7VlU6GKi/IcnAyJxNbTHIcPH9Zdd92lI0eORNx27NgxNTY26ve//72ef/55/c///I/GxsbsHBUAYl5HV78amnxnhIQkDY1MqqHJp46ufocmQ6KxNSaam5u1bds2eTyeiNsOHDig0tJSpaena9GiRfJ6vers7LRzVACIacGQpV0t3RHXPLOnR8GQZdNESGS2nubYsWPHjLYdP35cOTk54ccej0d+v3/a71tWVjbt1wYGBpSbmzvLSQEgtvX2DZ1zROJsn5w8pd6+IRUuybZpKiSqmLwAMyUlRaFQKPw4EAgoEAg4OBEAxJbh0cghMdt1gIn/KiZCoZD8fr8uuugihUIhJSfP7dmSnJwcdXf/5/Cd3+9XUVHRtOv37ds37dciHbUAgHiVlZE6p+sAE7OugK6uLq1fv1633Xabjh07prKyMvX29s7pUCUlJWpvb9f4+LhOnDghn88nr9c7pz8DAOJZQb5b7sWRQyE7M00F+W6bJkIim3VM/PKXv9Rzzz2nzMxMXXzxxXrsscf0yCOPzOlQbrdbtbW1qq6u1h133KG6urrwW0YBAJIrOUlbNhVGXLO5cin3m4AtZn2aY2JiQpdeemn48YoVKzQ5Obtzcq+99tqXbquqqlJVVdVsxwOAhFFclKftNd5z7jORnZmmzZVLuc8EbDPrmHC5XBocHFRS0r9rt7u7WykpMXkdJwDMe8VFeVqxNJc7YMJRs66A+++/XzU1NfL7/brjjjt06NAhPf7441EYDQAwE67kJN7+CUfNOiZWrFihF198UW+//bYsy9LVV1+tzMzMKIwGAADiwawvwHzvvff005/+VGvWrNGll16qu+++W4cPH47GbAAAIA7MOiYefvjh8IWRS5Ys0datW/XQQw/N+WAAACA+zDomJiYmtG7duvDj1atXa3x8fE6HAgAA8WPWMXHeeefprbfeCj9+++23tXDhwjkdCgAAxI9ZX4BZX1+ve++9V+np6ZL+/bkZjY2Ncz4YAACID7OOiWXLlun111/XoUOHlJKSovz8fJ133nnRmA0AAMSBGcfEK6+8ovLycr3wwgtnbP/8lEd1dfXcTgYAAOLCjGPi0KFDKi8vV09PTzTnAQAAcWbGMbF161ZJUkZGhrZv3x61gQAAQHyZ9bs5fD5fNOYAAABxatYXYHo8Ht1yyy1atmyZFixYEN7+4x//eE4HAwAA8WFWMTE2NiZJWrlypdLS0qIyEAAAiC8zjolXX31VDz74oNLS0tTT06Nnn31WX//616M5GwAAiAMzvmbiqaee0ksvvaSOjg7t3LlTTz75ZDTnAgAAcWJWF2BeccUVkqQbbrhB/f39URkIAADElxnHhGVZER8DAIDENOu3hgIAAPz/ZnwB5gcffKCVK1eGH4+OjmrlypWyLEtJSUk6cOBAVAYEAACxbcYx8ec//zmacwAAcI5gyFJv35CGRyeVlZGqgny3XMlJTo+Fs8w4Ji655JJozgEAwBk6uvq1q6VbQyOT4W3uxanasqlQxUV5Dk6Gs3HNBAAg5nR09auhyXdGSEjS0MikGpp86ujiHYWxhJgAAMSUYMjSrpbuiGue2dOjYIh3FcYKYgIAEFN6+4bOOSJxtk9OnlJv35BNE+HLEBMAgJgyPBo5JGa7DtFHTAAAYkpWRuqcrkP0ERMAgJhSkO+We3HkUMjOTFNBvtumifBliAkAQExxJSdpy6bCiGs2Vy7lfhMxhJgAAMSc4qI8ba/xnnOEIjszTdtrvNxnIsbM+KZVAADYqbgoTyuW5nIHzDhATAAAYpYrOUmFS7KdHgNfgtMcAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACPeZABwSDFkJfTOeRH/+wHxCTAAO6Ojq166Wbg2N/OcjlN2LU7VlU2FC3CY40Z8/MN9wmgOwWUdXvxqafGf8IZWkoZFJNTT51NHV79Bk9kj05w/MR8QEYKNgyNKulu6Ia57Z06NgyLJpInsl+vMH5itiArBRb9/QOf8iP9snJ0+pt2/IponslejPH5iviAnARsOjkf+QznZdvEn05w/MV8QEYKOsjNQ5XRdvEv35A/MVMQHYqCDfLffiyH8oszPTVJDvtmkieyX68wfmK2ICsJErOUlbNhVGXLO5cum8vd9Coj9/YL4iJgCbFRflaXuN95x/oWdnpml7jXfe32ch0Z8/MB9x0yrAAcVFeVqxNDdh7wCZ6M8fmG+ICcAhruQkFS7JdnoMxyT68wfmE05zAAAAI8QEAAAwQkwAAAAjtsfExMSENm7cqOHhYUlSa2urKioqVFVVpb1794bXTbcdAADEFlsvwDx8+LDq6+t15MgRSdKxY8fU2NiolpYWWZalqqoqrVq1SmNjY1+4PT093c5xAQDADNgaE83Nzdq2bZvq6+slSQcOHFBpaWk4Erxerzo7O/Xpp59+4fZ169Z94fctKyub9mcODAwoNzd3jp8JAAD4nK0xsWPHjjMeHz9+XDk5OeHHHo9Hfr9fIyMjX7gdAADEHkfvM5GSkqJQKBR+HAgEFAgEpt0+nX379k37tUhHLQAAgDlH382Rk5OjwcHB8GO/3y+PxzPtdgCYT4IhS90ffqK//u/H6v7wEwVDltMjAf8VR49MlJSU6KmnntL4+Limpqbk8/lUV1cny7K+cDsAzBcdXf3a1dKtoZHJ8Db34lRt2VTI55Mg7jgaE263W7W1taqurlYwGFRdXZ2ysrIkadrtABDvOrr61dDkO2f70MikGpp8fOAZ4k6SZVnz+rja59dMRLquAgDsEgxZuvv//PmMIxJny85M07P16/ngMzhupn9DuQMmANiot28oYkhI0icnT6m3b8imiQBzxAQA2Gh4NHJIzHYdEAuICQCwUVZG6pyuA2IBMQEANirId8u9OHIoZGemqSDfbdNEgDliAgBs5EpO0pZNhRHXbK5cysWXiCvEBADYrLgoT9trvOccocjOTONtoYhLjt5nAgASVXFRnlYszVVv35CGRyeVlZGqgnw3RyQQl4gJAHCIKzlJhUuynR4DMMZpDgAAYISYAAAARogJAABghJgAAABGiAkAAGCEmAAAAEaICQAAYIT7TAAIC4YsbqIEYNaICQCSpI6ufu1q6dbQyH8++tq9OFVbNhVye2cAEXGaA4A6uvrV0OQ7IyQkaWhkUg1NPnV09Ts0GYB4QEwACS4YsrSrpTvimmf29CgYsmyaCEC8ISaABNfbN3TOEYmzfXLylHr7hmyaCEC8ISaABDc8GjkkZrsOQOIhJoAEl5WROqfrACQeYgJIcAX5brkXRw6F7Mw0FeS7bZoIQLwhJoAE50pO0pZNhRHXbK5cyv0mAEyLmACg4qI8ba/xnnOEIjszTdtrvNxnAkBE3LQKgKR/B8WKpbncARPArBETAMJcyUkqXJLt9BgA4gynOQAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBFiAgAAGCEmAACAEWICAAAYISYAAIARYgIAABghJgAAgBE+NXSWgiGLj2hGTOE1CcBpxMQsdHT1a1dLt4ZGJsPb3ItTtWVToYqL8hycDImK1ySAWMBpjhnq6OpXQ5PvjF/akjQ0MqmGJp86uvodmgyJitckgFhBTMxAMGRpV0t3xDXP7OlRMGTZNBESHa9JALGEmJiB3r6hc/71d7ZPTp5Sb9+QTRMh0fGaBBBLiIkZGB6N/Et7tusAU7wmAcQSYmIGsjJS53QdYIrXJIBYQkzMQEG+W+7FkX8pZ2emqSDfbdNESHS8JgHEEmJiBlzJSdqyqTDims2VS3lvP2zDaxJALCEmZqi4KE/ba7zn/GswOzNN22u8vKcftuM1CSBWcNOqWSguytOKpbncbRAxg9ckgFhATMySKzlJhUuynR4DCOM1CcBpnOYAAABGiAkAAGCEmAAAAEZiIiYaGxt144036tZbb9Wrr74qSWptbVVFRYWqqqq0d+9ehycEAADTcfwCzPb2drW3t6ulpUXBYFDf+9739I1vfEONjY1qaWmRZVmqqqrSqlWrlJ6e7vS4AADgLI7HxPvvv681a9YoNfXf75W/6qqrtHv3bpWWlobjwev1qrOzU+vWrfvC71FWVjbt9x8YGFBubu7cDw4AACTFwGmOgoICtbW1aXx8XCdOnNDBgwflcrmUk5MTXuPxeOT3+x2cEgAATMfxIxPFxcXq6enR7bffrksuuUTXXnutXC7XGWsCgYACgcC032Pfvn3Tfi3SUQsAAGDO8SMT4+PjqqysVGtrq55++mmNj4/L4/FocHAwvMbv98vj8Tg4JQAAmI7jMXH06FFt3bpVoVBIR48e1cGDB1VSUqL29vbwqQ+fzyev1+v0qAAA4As4fpqjoKBAy5cv10033aRQKKRHH31U2dnZqq2tVXV1tYLBoOrq6pSVleX0qAAA4AskWZZlOT1ENH1+zUSk6yoAAMC5Zvo31PHTHAAAIL4REwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwEiK0wMAsSIYstTbN6Th0UllZaSqIN8tV3KS02MBQMwjJgBJHV392tXSraGRyfA29+JUbdlUqOKiPAcnA4DYx2kOJLyOrn41NPnOCAlJGhqZVEOTTx1d/Q5NBgDxgZhAQguGLO1q6Y645pk9PQqGLJsmAoD4Q0wgofX2DZ1zROJsn5w8pd6+IZsmAoD4Q0wgoQ2PRg6J2a4DgERETCChZWWkzuk6AEhExAQSWkG+W+7FkUMhOzNNBflumyYCgPhDTCChuZKTtGVTYcQ1myuXcr8JAIiAmEDCKy7K0/Ya7zlHKLIz07S9xst9JgDgS3DTKkD/DooVS3O5AyYA/BeICeD/cSUnqXBJttNjAEDc4TQHAAAwQkwAAAAjxAQAADBCTAAAACPEBAAAMEJMAAAAI/P+raGDg4MKBoMqKytzehQAAOLKwMCAXC7Xl66b90cmFi5cqJSUed9MGhgY0MDAgNNjJAz2t33Y1/Zif9sr1vd3SkqKFi5c+KXrkizLsmyYB1H2+ZGXffv2OTxJYmB/24d9bS/2t73my/6e90cmAABAdBETAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAhvDQUAAEY4MgEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBNx6rXXXtMNN9yg8vJy/epXv5Iktba2qqKiQlVVVdq7d6/DE84fLS0tqqysDP/vm9/8pp599ln2dxT95je/UXl5uW666SY1NzdL4vUdTY2Njbrxxht166236tVXX5XE/p5rExMT2rhxo4aHhyVNv3/jdr9biDsnT560vv3tb1sfffSRNTU1Zd18883WwYMHrbKyMuvTTz+1RkdHw/8fc6unp8fasGGD9fHHH7O/o2RgYMBau3atNTExYY2NjVlr1661BgYG2N9R0tbWZn3nO9+xTp06ZY2NjVk333wzr+859uGHH1q33XabdeWVV1pDQ0PTvp7j+XXOkYk49Le//U3XXnutLr/8cp133nn69a9/rX/9618qLS1Venq6Fi1aJK/Xq87OTqdHnXd27typ+vp6vfnmm+zvKElOTlYoFNJnn32m06dPKxAI6MCBA+zvKHn//fe1Zs0apaam6oILLtBVV12l3bt3s7/nUHNzs7Zt2yaPxyNJ076e4/l1TkzEoY8//ljp6em69957VVlZqb179+r48ePKyckJr/F4PPL7/Q5OOf+0tbVp4cKFKi4uZn9HkcfjUXl5ua677jqtXbtWlZWV7O8oKigoUFtbm8bHx3XixAkdPHhQLpeL/T2HduzYoZUrV4YfT/d6jufXeYrTA2D2AoGA9u/fr5deekmLFi1STU2N1q9fr+Tk5DPWBAIBB6ecf5qbm3X77bdLklJSUhQKhcJfY3/PHZ/Pp87OTu3fv1+hUEg//OEPdf311/P6jpLi4mL19PTo9ttv1yWXXKJrr71WLpfrjDXs77k13e+PeP69wpGJOJSdna1rrrlGF110kc4//3ytXr1agUBAg4OD4TV+vz98SA3mpqam9Pe//13XXXedJCknJ4f9HSXvvPOOysrKlJGRoczMTK1Zs0anT59mf0fJ+Pi4Kisr1draqqefflrj4+PyeDzs7yia7vdHPP9eISbi0MqVK/X222/r5MmTOn36tA4cOKDLL79c7e3t4UOVPp9PXq/X6VHnjX/+85/6yle+okWLFkmSSkpK2N9RcuWVV6qjo0NTU1OamppSR0eH8vLy2N9RcvToUW3dulWhUEhHjx7VwYMHeX1H2XT7N573O6c54tCll16qBx98UDU1NQoGg7r++uu1YcMGffbZZ6qurlYwGFRdXZ2ysrKcHnXeOHbsmC6//PLwY7fbrdraWvZ3FBQXF+udd97RLbfcIsuyVF5erqqqKklif0dBQUGBli9frptuukmhUEiPPvqosrOzeX1HUaTfH/G63/nUUAAAYITTHAAAwAgxAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxASBqxsbGdPXVV+tnP/uZ06MAiCJiAkDU7NmzR2vXrlVra6s+/fRTp8cBECXcThtA1Lz88su69957NTIyoj/+8Y+qqanRE088oZ6eHh0/flxer1cPPPCAGhoa1NXVJZfLpWXLluknP/mJXC6Xdu/erd/+9rc6ffq0Tp06pQceeEDl5eVOPy0AZyEmAETFu+++q/7+fq1atUoTExN68skndccdd0iSBgcH9Yc//EEul0uNjY3KzMzU7t27ZVmWdu7cqRdffFGVlZV66aWX1NTUpEWLFukvf/mLnnjiCWICiEHEBICoePHFF1VeXq4FCxZo/fr1euihh9TW1iZJWrZsmVwulyTpr3/9q8bHx7V//35J0uTkpFJTU5Wenq6nn35abW1t+uCDD/Tuu+9qbGzMqacDIAJiAsCcGxsb0yuvvKILL7xQN954oyTJ5XLpd7/7nYqKirRgwYIz1j/yyCNasWKFJGlkZEQul0v9/f36wQ9+oOrqahUXF2v58uXauXOn7c8FwJcjJgDMuT179uiyyy7Tn/70p/C2Q4cOqaKiQhkZGfJ4POHtpaWleuGFF3TNNddIku677z6tX79e2dnZ8ng8uvvuuxUMBvXII48oGAza/lwAfDnezQFgzr388su68847z9j21a9+VWvWrNGFF154xvZ77rlHmZmZqqysVEVFhb72ta/ptttuU0lJiTIzM1VeXq4777xThYWFGh0d5V0hQAxKsizLcnoIAAAQvzgyAQAAjBATAADACDEBAACMEBMAAMAIMQEAAIwQEwAAwAgxAQAAjBATAADACDEBAACMEBMAAMDI/wVk2oaZ3/aSpQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如上面所说，线性回归即通过线性方程去拟合数据点。那么，我们可以令该 1 次函数的表达式为：\n",
    "$$y(x, w) = w_0 + w_1x \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. 平方损失函数 (square loss)\n",
    "\n",
    "正如上面所说，如果一个数据点为 ($x_{i}$, $y_{i}$)，那么它对应的误差 (Error) 就为:\n",
    "\n",
    "$$y_{i}-(w_0 + w_1x_{i}) \\tag{2}$$\n",
    "\n",
    "\n",
    "上面的误差往往也称之为「 **残差**」 (Residual)。但是在机器学习中，我们更喜欢称作「 **损失** 」 (Loss)，即真实值和预测值之间的偏离程度。那么，对 $n$ 个全部数据点而言，其对应的残差损失总和就为：\n",
    "\n",
    "$$\\sum\\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}}\n",
    "                    \\tag{3}$$\n",
    "\n",
    "更进一步，在线性回归中，我们一般使用残差的平方和来表示所有样本点的误差。公式如下：\n",
    "\n",
    "$$\\sum\\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}^2}\n",
    "                    \\tag{4}$$\n",
    "\n",
    "- 残差：$r_i = y_i - \\hat{y}_i$\n",
    "- 平方损失函数：$L(y, \\hat{y}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. 最小二乘法代数求解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乘法是用于求解线性回归拟合参数 $w$ 的一种常用方法。最小二乘法中的「二乘」代表平方，最小二乘也就是最小平方。而这里的平方就是指代上面的平方损失函数。\n",
    "\n",
    "简单来讲，最小二乘法也就是求解平方损失函数最小值的方法。那么，到底该怎样求解呢？这就需要使用到高等数学中的知识。推导如下：\n",
    "\n",
    "首先，平方损失函数为：\n",
    "$$f = \\sum\\limits_{i = 1}^n {{{(y_{i}-(w_0 +\n",
    "                    w_1x_{i}))}}^2} \\tag{5}$$\n",
    " \n",
    "我们的目标是求取平方损失函数 $min(f)$ 最小时，对应的 $w$。首先求 $f$ 的 1 阶偏导数：\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial\n",
    "                    w_{0}}=-2(\\sum_{i=1}^{n}{y_i}-nw_{0}-w_{1}\\sum_{i=1}^{n}{x_i})\n",
    "                    \\tag{6a}$$\n",
    " \n",
    "$$\\frac{\\partial f}{\\partial\n",
    "                    w_{1}}=-2(\\sum_{i=1}^{n}{x_iy_i}-w_{0}\\sum_{i=1}^{n}{x_i}-w_{1}\\sum_{i=1}^{n}{x_i}^2)\n",
    "                    \\tag{6b}$$\n",
    "然后，我们令 $\\frac{\\partial f}{\\partial w_{0}}=0$ 以及 $\\frac{\\partial f}{\\partial w_{1}}=0$，解得：\n",
    "\n",
    "$$w_{1}=\\frac\n",
    "                    {n\\sum_{}^{}{x_iy_i}-\\sum_{}^{}{x_i}\\sum_{}^{}{y_i}}\n",
    "                    {n\\sum_{}^{}{x_i}^2-(\\sum_{}^{}{x_i})^2} \\tag{7b}$$ \n",
    " \n",
    "$$w_{0}=\\frac\n",
    "                    {\\sum_{}^{}{x_i}^2\\sum_{}^{}{y_i}-\\sum_{}^{}{x_i}\\sum_{}^{}{x_iy_i}}\n",
    "                    {n\\sum_{}^{}{x_i}^2-(\\sum_{}^{}{x_i})^2} \\tag{7b}$$\n",
    " \n",
    "到目前为止，已经求出了平方损失函数最小时对应的 $w$ 参数值，这也就是最佳拟合直线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. 最小二乘法矩阵求解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习完上面的内容，相信你已经了解了什么是最小二乘法，以及如何使用最小二乘法进行线性回归拟合。上面，我们采用了求偏导数的方法，并通过代数求解找到了最佳拟合参数 $w$ 的值。这里尝试另外一种方法，即通过矩阵的变换来计算参数 $w$。\n",
    "\n",
    "首先，一元线性函数的表达式为 $y(x, w) = w_0 + w_1x$，表达成矩阵形式为：\n",
    "\n",
    "$$\\begin{split} \\left[ \\begin{array}{c}{1, x_{1}} \\\\ {1,\n",
    "                    x_{2}} \\\\ {\\cdots} \\\\ {1, x_{9}} \\\\ {1,\n",
    "                    x_{10}}\\end{array}\\right] \\left[ \\begin{array}{c}{w_{0}} \\\\\n",
    "                    {w_{1}}\\end{array}\\right] = \\left[ \\begin{array}{c}{y_{1}}\n",
    "                    \\\\ {y_{2}} \\\\ {\\cdots} \\\\ {y_{9}} \\\\\n",
    "                    {y_{10}}\\end{array}\\right] \\Rightarrow \\left[\n",
    "                    \\begin{array}{c}{1,56} \\\\ {1,72} \\\\ {\\cdots} \\\\ {1,94} \\\\\n",
    "                    {1,74}\\end{array}\\right] \\left[ \\begin{array}{c}{w_{0}} \\\\\n",
    "                    {w_{1}}\\end{array}\\right]=\\left[ \\begin{array}{c}{92} \\\\\n",
    "                    {102} \\\\ {\\cdots} \\\\ {105} \\\\ {92}\\end{array}\\right]\n",
    "                    \\end{split}$$\n",
    " \n",
    "即：\n",
    "$$y(x, w) = XW \\tag{8b}$$\n",
    " \n",
    "$(8)$ 式中， $W$ 为 $\\begin{bmatrix}w_{0} \\\\ w_{1} \\end{bmatrix}$，而 $X$ 则是 $\\begin{bmatrix}1, x_{1} \\\\ 1, x_{2} \\\\ \\cdots \\\\ 1,\n",
    "                      x_{9} \\\\ 1, x_{10} \\end{bmatrix}$ 矩阵。然后，平方损失函数为：\n",
    "\n",
    "$$f = \\sum\\limits_{i = 1}^n {{{(y_{i}-(w_0 +\n",
    "                    w_1x_{i}))}}}^2 =(y-XW)^T(y-XW)\\tag{9}$$\n",
    " \n",
    "通过对公式 $(9)$ 实施矩阵计算乘法分配律得到：\n",
    "\n",
    "$$f = y^{T}y - y^{T}(XW) - (XW)^{T}y + (XW)^{T}(XW)\n",
    "                    \\tag{10}$$\n",
    " \n",
    "在该公式中 $y$ 与 $XW$ 皆为相同形式的 $(m,1)$ 矩阵，由此两者相乘属于线性关系，所以等价转换如下：\n",
    "\n",
    "$$\\begin{split} f = y^{T}y - (XW)^{T}y - (XW)^{T}y +\n",
    "                    (XW)^{T}(XW)\\\\ = y^{T}y - 2 (XW)^{T}y + (XW)^{T}(XW)\n",
    "                    \\end{split}$$\n",
    " \n",
    "此时，对 [矩阵求偏导数](https://en.wikipedia.org/wiki/Matrix_calculus) 得到：\n",
    "$$\\frac{\\partial f}{\\partial W}=2X^TXW-2X^Ty=0 \\tag{12}$$\n",
    " \n",
    " \n",
    "当矩阵 $X^TX$ 满秩时，$(X^TX)^{-1}X^TX=E$ ，且 $EW=W$。所以有 $(X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty$，并最终得到：\n",
    "$$W=(X^TX)^{-1}X^Ty \\tag{13}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. 线性回归 scikit-learn 实现\n",
    "\n",
    "```python\n",
    "sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "# - fit_intercept: 默认为 True，计算截距项。\n",
    "# - normalize: 默认为 False，不针对数据进行标准化处理。\n",
    "# - copy_X: 默认为 True，即使用数据的副本进行操作，防止影响原数据。\n",
    "# - n_jobs: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.33509168550617, array([0.75458428]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义线性回归模型\n",
    "model = LinearRegression()\n",
    "model.fit(x.reshape(x.shape[0], 1), y)  # 训练, reshape 操作把数据处理成 fit 能接受的形状\n",
    "\n",
    "# 得到模型拟合参数\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([154.52273298])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. 线性回归综合案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "   LSTAT  MEDV  \n",
       "0   4.98  24.0  \n",
       "1   9.14  21.6  \n",
       "2   4.03  34.7  \n",
       "3   2.94  33.4  \n",
       "4   5.33  36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://github.com/NishadKhudabux/Boston-House-Price-Prediction/raw/refs/heads/main/Boston.csv'\n",
    "# column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "df = pd.read_csv(url, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'LSTAT', 'MEDV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>rm</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim          rm       lstat\n",
       "count  506.000000  506.000000  506.000000\n",
       "mean     3.613524    6.284634   12.653063\n",
       "std      8.601545    0.702617    7.141062\n",
       "min      0.006320    3.561000    1.730000\n",
       "25%      0.082045    5.885500    6.950000\n",
       "50%      0.256510    6.208500   11.360000\n",
       "75%      3.677083    6.623500   16.955000\n",
       "max     88.976200    8.780000   37.970000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [x.lower() for x in df.columns]\n",
    "\n",
    "features = df[[\"crim\", \"rm\", \"lstat\"]]\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 3) (354,) (152, 3) (152,)\n"
     ]
    }
   ],
   "source": [
    "target = df[\"medv\"]  # 目标值数据\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 将特征和目标值分开\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# 使用 train_test_split 分割数据，将 30% 的数据作为测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 查看数据集的形状\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.12362012,  5.09661773, -0.60817459]), -1.2405510324250422)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()  # 建立模型\n",
    "model.fit(X_train, y_train)  # 训练模型\n",
    "model.coef_, model.intercept_  # 输出训练后的模型参数和截距项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的单元格中，我们输出了线性回归模型的拟合参数。也就是最终的拟合线性函数近似为：\n",
    "\n",
    "$$f = -0.1236 * x_{1} + 5.0966 * x_{2} -0.6081 * x_{3} -1.2405 \\tag{14}$$\n",
    "\n",
    "其中，$x_{1}$, $x_{2}$ 和 $x_{3}$ 分别对应数据集中 `CRIM`, `RM` 和 `LSTAT` 列。接下来，向训练好的模型中输入测试集的特征得到预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.95010416, 31.04855815, 18.2497264 , 26.3019832 , 19.72042823,\n",
       "       23.46871069, 17.71366141, 15.49650764, 22.17984511, 20.73388426,\n",
       "       18.17724891, 18.93873596, -6.35437003, 23.00971662, 20.67587505,\n",
       "       26.66807591, 18.01857996,  3.23670389, 37.08732784, 18.10256111,\n",
       "       26.53443298, 27.41308736, 13.99188946, 26.47265903, 18.95326059,\n",
       "       14.39064862, 23.15222471, 20.49447042, 18.61054869, 19.6808132 ,\n",
       "       18.20651772, 27.11073871, 25.4330687 , 19.43551217, 15.84148017,\n",
       "       17.98728092, 32.92433015, 22.70715097, 20.73270312, 25.97803613,\n",
       "       13.33760731, 29.08053218, 37.95164944, 19.29216769, 26.10063642,\n",
       "       16.60989702, 16.51288715, 27.36660097, 19.7455285 , 29.24021979,\n",
       "       21.30002643, 31.4765798 , 18.52548865, 28.64078242, 34.84253501,\n",
       "       24.04067339, 19.66522085, 31.69779263, 25.46484613, 16.07543617,\n",
       "       27.42948149, 32.77789044, 29.81113896, 19.35780323, 28.9413907 ,\n",
       "       11.89916031, 20.25014766, 26.78687738, 29.74091303, 17.07827625,\n",
       "       19.61952361, 27.74870143, 12.54395385, 25.61226266, 23.79596517,\n",
       "        3.20619077, 22.67424043, 36.46170373, 17.33409647, 11.75192421,\n",
       "       23.3913465 , 10.41607184, 23.07717374,  7.37683104, 22.2962212 ,\n",
       "       28.08812367, 22.33078153, 27.69005761, 26.63422339, 22.73619744,\n",
       "       23.28376198,  6.38335834, 23.39696086, 21.1061108 , 12.01510444,\n",
       "       24.39925364, 23.50282292, -0.88178284, 18.92844379, 18.04521922,\n",
       "       21.58806596, 25.34794842,  8.78508592, 23.26431999, 24.73085877,\n",
       "       14.82814703, 20.99784584, 28.4225529 , 24.65135513, 27.77945214,\n",
       "       12.15141611, 19.86691593, 26.80930743, 24.31730346, 31.34755289,\n",
       "       19.45326717, 33.72874163, 17.1505415 , 20.52024491, 27.91244682,\n",
       "       19.63717391, 28.21563418,  7.48914676, 23.16332186, 26.64949711,\n",
       "       24.82176435, 28.09870799, 32.28309414, 20.54206054, 36.32384001,\n",
       "       10.74004071, 26.56401533, 21.60804452, 19.38688193,  7.67884687,\n",
       "       23.39226334, 24.46020314, 31.09393553, 29.75192884, 18.94581935,\n",
       "       20.26932503, 28.06686746, 22.02045463, 11.67109054,  5.16317683,\n",
       "       24.16233599, 20.62623892, 18.33470988, 16.17338714, 40.34645114,\n",
       "       21.0067563 , 19.39310502])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test)  # 输入测试集特征进行预测\n",
    "preds  # 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn MAE:  4.111995393754922\n",
      "scikit-learn MSE:  29.975964330767464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae_ = mean_absolute_error(y_test, preds)\n",
    "mse_ = mean_squared_error(y_test, preds)\n",
    "\n",
    "print(\"scikit-learn MAE: \", mae_)\n",
    "print(\"scikit-learn MSE: \", mse_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
